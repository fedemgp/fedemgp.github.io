<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Schema infering - PySpark data mocker</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Schema infering";
        var mkdocs_page_input_path = "documentation/schema_infering.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/python.min.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/languages/bash.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> PySpark data mocker
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Documentation</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../configuration/">Configuration</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="./">Schema infering</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#automatic-inferring">Automatic inferring</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#schema-configuration-file">Schema configuration file</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#example">Example</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#combining-both-schema-inferring-configurations">Combining both schema inferring configurations</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#column-types">Column types</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#example_1">Example</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#limitations">Limitations</a>
    </li>
        </ul>
    </li>
    </ul>
                  </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">PySpark data mocker</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a> &raquo;</li>
          <li>Documentation &raquo;</li>
      <li class="breadcrumb-item active">Schema infering</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <!--
# To improve the naming of the datalake and avoid refactor the project, move the basic datalake temporally
$ mv tests/data/datalake_with_config_schema/bar tests/data/datalake_with_config_schema/school
$ mv tests/data/datalake_with_config_schema/foo tests/data/datalake_with_config_schema/grades
$ mv tests/data/basic_datalake/bar tests/data/basic_datalake/school
$ mv tests/data/basic_datalake/foo tests/data/basic_datalake/grades
$ echo "school.courses:
>   id: int
>   course_name: string
> school.students:
>   id: int
>   first_name: string
>   last_name: string
>   email: string
>   gender: string
>   birth_date: date
> " > tests/data/datalake_with_config_schema/schema_config.yaml
-->
<h1 id="schema-inferring">Schema inferring</h1>
<p><code>pyspark-data-mocker</code> lets you define the schema of the table as you please. You can enable automatic schema inferring
by setting up the <code>schema.infer</code> option in the <a href="https://fedemgp.github.io/documentation/configuration#configuration-file-explanation">configuration file</a>,
or you can manually specify the schema of each column you want using another yaml file. By default,
<code>pyspark-data-mocker</code> will consider that all columns are <code>string</code> columns</p>
<h2 id="automatic-inferring">Automatic inferring</h2>
<p>This is the simplest configuration. Let's see the example we saw before in the <a href="https://fedemgp.github.io">welcome page</a>
with automatic infer schema enabled</p>
<pre><code class="language-bash">$ cat ./tests/data/config/infer_schema.yaml
app_name: &quot;test&quot;
number_of_cores: 1
schema:
  infer: True
</code></pre>
<p>You only need to set the boolean <code>schema.infer</code> to <code>True</code> and that is it! once you load the builder, the columns
will vary depending on their values</p>
<pre><code class="language-python">&gt;&gt;&gt; from pyspark_data_mocker import DataLakeBuilder
&gt;&gt;&gt; builder = DataLakeBuilder.load_from_dir(&quot;./tests/data/basic_datalake&quot;, &quot;./tests/data/config/infer_schema.yaml&quot;)  # byexample: +timeout=20 +pass
&gt;&gt;&gt; spark = builder.spark
&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE school.students&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+----------+---------+
|  col_name|data_type|
+----------+---------+
|        id|      int|
|first_name|   string|
| last_name|   string|
|     email|   string|
|    gender|   string|
|birth_date|   string|
+----------+---------+


&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE school.courses&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+-----------+---------+
|   col_name|data_type|
+-----------+---------+
|         id|      int|
|course_name|   string|
+-----------+---------+


&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE grades.exams&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+----------+---------+
|  col_name|data_type|
+----------+---------+
|        id|      int|
|student_id|      int|
| course_id|      int|
|      date|   string|
|      note|      int|
+----------+---------+

&gt;&gt;&gt; builder.cleanup()
</code></pre>
<h2 id="schema-configuration-file">Schema configuration file</h2>
<p>This yaml file needs to be located in the folder you will place the datalake definition (the root path you will
pass to the <code>DatalakeBuilder</code> class). By default, the config file that will be used is called <code>schema_config.yaml</code>,
but it can be overriden in the <a href="https://fedemgp.github.io/documentation/configuration#configuration-file-explanation">application configuration file</a>.</p>
<pre><code class="language-bash">$ cat ./pyspark_data_mocker/config/schema.py
&lt;...&gt;schema.Optional(&quot;config_file&quot;, default=&quot;schema_config.yaml&quot;)&lt;...&gt;
</code></pre>
<p>That yaml needs to be a file where each key represents the table name (considering the database), and as value,
a dictionary with the columns as keys, and a <a href="https://vincent.doba.fr/posts/20211004_spark_data_description_language_for_defining_spark_schema/">Spark's DDL type of the column</a> as value.</p>
<h2 id="example">Example</h2>
<p>Let's consider this datalake definition.</p>
<pre><code class="language-bash">$ tree tests/data/datalake_with_config_schema -n --charset=ascii  # byexample: +rm=~
tests/data/datalake_with_config_schema
|-- grades
|   `-- exams.csv
|-- schema_config.yaml
`-- school
    |-- courses.csv
    `-- students.csv
~
2 directories, 4 files
</code></pre>
<p>Notice how in this example, unlike the one seen previously in the <a href="https://fedemgp.github.io">Home section</a> contains a
file <code>schema_config.yaml</code>. The content of this file will define the types of each column of the tables.</p>
<pre><code class="language-bash">$ cat tests/data/datalake_with_config_schema/schema_config.yaml
school.courses:
  id: int
  course_name: string
school.students:
  id: int
  first_name: string
  last_name: string
  email: string
  gender: string
  birth_date: date
</code></pre>
<p>Take a moment to digest the schema of the file. How each key of the yaml dictionary is the full name of the table
that will be created, and as value contains another dictionary with the columns of the table, and the type of that column.
Let's build up the datalake</p>
<pre><code class="language-python">&gt;&gt;&gt; builder = DataLakeBuilder.load_from_dir(&quot;./tests/data/datalake_with_config_schema&quot;)  # byexample: +timeout=20 +pass
&gt;&gt;&gt; spark = builder.spark

&gt;&gt;&gt; spark.sql(&quot;SHOW TABLES IN school&quot;).show()
+---------+---------+-----------+
|namespace|tableName|isTemporary|
+---------+---------+-----------+
|   school|  courses|      false|
|   school| students|      false|
+---------+---------+-----------+


&gt;&gt;&gt; spark.sql(&quot;SHOW TABLES IN grades&quot;).show()
+---------+---------+-----------+
|namespace|tableName|isTemporary|
+---------+---------+-----------+
|   grades|    exams|      false|
+---------+---------+-----------+
</code></pre>
<p>Now the tables are loaded, we can take a look at the schema of each table.</p>
<pre><code class="language-python">&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE school.students&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+----------+---------+
|  col_name|data_type|
+----------+---------+
|        id|      int|
|first_name|   string|
| last_name|   string|
|     email|   string|
|    gender|   string|
|birth_date|     date|
+----------+---------+


&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE school.courses&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+-----------+---------+
|   col_name|data_type|
+-----------+---------+
|         id|      int|
|course_name|   string|
+-----------+---------+


&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE grades.exams&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+----------+---------+
|  col_name|data_type|
+----------+---------+
|        id|   string|
|student_id|   string|
| course_id|   string|
|      date|   string|
|      note|   string|
+----------+---------+

&gt;&gt;&gt; builder.cleanup()
</code></pre>
<!--
>>> spark.sql("SHOW DATABASES").show()
+---------+
|namespace|
+---------+
|  default|
+---------+
-->

<p>Now the column types changed! we have the <code>birth_date</code> that is <code>date</code> type and the ids as <code>int</code>. Notice also that
the table <code>grades.exams</code> (which we didn't define any custom schema) has for each column the default value <code>string</code>
(because it's the fallback type as we saw before).</p>
<h2 id="combining-both-schema-inferring-configurations">Combining both schema inferring configurations</h2>
<p>We can combine this file with the automatic <code>infer</code> option to only configure manually the schemas that we need.</p>
<pre><code class="language-python">&gt;&gt;&gt; builder = DataLakeBuilder.load_from_dir(&quot;./tests/data/datalake_with_config_schema&quot;, &quot;./tests/data/config/infer_schema.yaml&quot;)  # byexample: +timeout=20 +pass
&gt;&gt;&gt; spark = builder.spark
&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE school.students&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+----------+---------+
|  col_name|data_type|
+----------+---------+
|        id|      int|
|first_name|   string|
| last_name|   string|
|     email|   string|
|    gender|   string|
|birth_date|     date|
+----------+---------+


&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE school.courses&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+-----------+---------+
|   col_name|data_type|
+-----------+---------+
|         id|      int|
|course_name|   string|
+-----------+---------+


&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE grades.exams&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+----------+---------+
|  col_name|data_type|
+----------+---------+
|        id|      int|
|student_id|      int|
| course_id|      int|
|      date|   string|
|      note|      int|
+----------+---------+

&gt;&gt;&gt; builder.cleanup()
</code></pre>
<p>Now the <code>grades.exams</code> table schema also changed! but take into consideration that the automatic schema inferring of spark
<strong>it's not magic</strong>. Note that the date column of <code>grades.exams</code> was not inferred to a <code>date</code> column type.
Sometimes it is needed to use the manual schema definition to have the value we need.</p>
<blockquote>
<p><strong>NOTE</strong>: This behavior is fixed starting from pyspark 3.3. From that version and beyond, it infers date columns, but
spark considers all date-kind values as datetime</p>
</blockquote>
<h2 id="column-types">Column types</h2>
<p>You can define the type of column of each type that <a href="https://spark.apache.org/docs/latest/sql-ref-datatypes.html">Spark supports</a>!
you don't have any restriction whatsoever (kind of, but more of that later). </p>
<h3 id="example_1">Example</h3>
<p>Consider these files and schema definitions</p>
<pre><code class="language-bash">$ tree tests/data/datalake_different_files_and_schemas -n --charset=ascii  # byexample: +rm=~
tests/data/datalake_different_files_and_schemas
|-- schema_config.yaml
`-- school
    |-- courses.json
    `-- students.csv
~
1 directory, 3 files

$ cat tests/data/datalake_different_files_and_schemas/school/courses.json
{&quot;id&quot;: 1, &quot;course_name&quot;: &quot;Algorithms 1&quot;, &quot;flags&quot;: {&quot;acitve&quot;: true}, &quot;correlative_courses&quot;: []}
{&quot;id&quot;: 2, &quot;course_name&quot;: &quot;Algorithms 2&quot;, &quot;flags&quot;: {&quot;acitve&quot;: true}, &quot;correlative_courses&quot;: [1]}

$ cat tests/data/datalake_different_files_and_schemas/schema_config.yaml
school.courses:
  id: int
  course_name: string
  flags: map&lt;string, boolean&gt;
  correlative_courses: array&lt;int&gt;
school.students:
  id: long
  name: string
  birth_date: date
</code></pre>
<p><code>pyspark-data-mocker</code> does not need that all files are in the same file format, it will infer each file depending on
the file extension (the limitation is that the file format is a valid spark source). Let's see the schemas and data
in each table.</p>
<pre><code class="language-python">&gt;&gt;&gt; builder = DataLakeBuilder.load_from_dir(&quot;./tests/data/datalake_different_files_and_schemas&quot;)  # byexample: +timeout=20 +pass
&gt;&gt;&gt; spark = builder.spark
&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE school.students&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+----------+---------+
|  col_name|data_type|
+----------+---------+
|        id|   bigint|
|      name|   string|
|birth_date|     date|
+----------+---------+

&gt;&gt;&gt; spark.table(&quot;school.students&quot;).show()
+---+----------------+----------+
| id|            name|birth_date|
+---+----------------+----------+
|  1|Shirleen Dunford|1978-08-01|
|  2|    Niko Puckrin|2000-11-28|
|  3|   Sergei Barukh|1992-01-20|
|  4|     Sal Maidens|2003-12-14|
|  5|Cooper MacGuffie|2000-03-07|
+---+----------------+----------+



&gt;&gt;&gt; spark.sql(&quot;DESCRIBE TABLE school.courses&quot;).select(&quot;col_name&quot;, &quot;data_type&quot;).show()
+-------------------+-------------------+
|           col_name|          data_type|
+-------------------+-------------------+
|                 id|                int|
|        course_name|             string|
|              flags|map&lt;string,boolean&gt;|
|correlative_courses|         array&lt;int&gt;|
+-------------------+-------------------+

&gt;&gt;&gt; spark.table(&quot;school.courses&quot;).show()
+---+------------+----------------+-------------------+
| id| course_name|           flags|correlative_courses|
+---+------------+----------------+-------------------+
|  1|Algorithms 1|{acitve -&gt; true}|                 []|
|  2|Algorithms 2|{acitve -&gt; true}|                [1]|
+---+------------+----------------+-------------------+


&gt;&gt;&gt; builder.cleanup()
</code></pre>
<h3 id="limitations">Limitations</h3>
<p>The limitation you have is that you cannot define certain column types in some file formats.
An example of that is to define a <code>map</code> column type in a <code>csv</code> file.</p>
<pre><code class="language-bash">$ cat tests/data/column_type_not_supported/schema_config.yaml
foo.bar:
  id: int
  invalid_col: map&lt;string, string&gt;

$ tree tests/data/column_type_not_supported -n --charset=ascii  # byexample: +rm=~
tests/data/column_type_not_supported
|-- foo
|   `-- bar.csv
`-- schema_config.yaml
~
1 directory, 2 files
</code></pre>
<p>If we want to set up this datalake, it will fail with this exception message.</p>
<pre><code class="language-python">&gt;&gt;&gt; builder = DataLakeBuilder.load_from_dir(&quot;./tests/data/column_type_not_supported&quot;)  # byexample: +timeout=20
&lt;...&gt;AnalysisException: CSV data source does not support map&lt;string,string&gt; data type.
</code></pre>
<!--
$ mv tests/data/datalake_with_config_schema/school tests/data/datalake_with_config_schema/bar
$ mv tests/data/datalake_with_config_schema/grades tests/data/datalake_with_config_schema/foo
$ mv tests/data/basic_datalake/school tests/data/basic_datalake/bar
$ mv tests/data/basic_datalake/grades tests/data/basic_datalake/foo
$ echo "bar.courses:
>   id: int
>   course_name: string
> bar.students:
>   id: int
>   first_name: string
>   last_name: string
>   email: string
>   gender: string
>   birth_date: date" > tests/data/datalake_with_config_schema/schema_config.yaml
-->
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../configuration/" class="btn btn-neutral float-left" title="Configuration"><span class="icon icon-circle-arrow-left"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../configuration/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
