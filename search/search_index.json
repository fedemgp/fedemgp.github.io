{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home pyspark-data-mocker is a testing tool that facilitates the burden of setting up a desired datalake, so you can test easily the behavior of your data application. It configures also the spark session to optimize it for testing purpose. Install pip install pyspark-data-mocker Basic usage pyspark-data-mocker searches the directory you provide in order to seek and load files that can be interpreted as tables, storing them inside the datalake. That datalake will contain certain databases depending on the folders inside the root directory. For example, let's take a look into the basic_datalake $ tree tests/data/basic_datalake -n --charset=ascii # byexample: +rm=~ +skip tests/data/basic_datalake |-- grades | `-- exams.csv `-- school |-- courses.csv `-- students.csv ~ 2 directories, 3 files This file hierarchy will be respected in the further datalake when loaded: each sub-folder will be considered as spark database, and each file will be loaded as table, using the filename to name the table. How can we load them using pyspark-data-mocker ? Really simple! >>> from pyspark_data_mocker import DataLakeBuilder >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/basic_datalake\") # byexample: +timeout=20 +pass And that's it! you will now have in that execution context a datalake with the structure defined in the folder basic_datalake . Let's take a closer look by running some queries. >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.getOrCreate() >>> spark.sql(\"SHOW DATABASES\").show() +---------+ |namespace| +---------+ | default| | grades| | school| +---------+ We have the default database (which came for free when instantiating spark), and the two folders inside tests/data/basic_datalake : school and grades . >>> spark.sql(\"SHOW TABLES IN school\").show() +---------+---------+-----------+ |namespace|tableName|isTemporary| +---------+---------+-----------+ | school| courses| false| | school| students| false| +---------+---------+-----------+ >>> spark.sql(\"SELECT * FROM school.courses\").show() +---+------------+ | id| course_name| +---+------------+ | 1|Algorithms 1| | 2|Algorithms 2| | 3| Calculus 1| +---+------------+ >>> spark.table(\"school.students\").show() +---+----------+---------+--------------------+------+----------+ | id|first_name|last_name| email|gender|birth_date| +---+----------+---------+--------------------+------+----------+ | 1| Shirleen| Dunford|sdunford0@amazona...|Female|1978-08-01| | 2| Niko| Puckrin|npuckrin1@shinyst...| Male|2000-11-28| | 3| Sergei| Barukh|sbarukh2@bizjourn...| Male|1992-01-20| | 4| Sal| Maidens|smaidens3@senate.gov| Male|2003-12-14| | 5| Cooper|MacGuffie| cmacguffie4@ibm.com| Male|2000-03-07| +---+----------+---------+--------------------+------+----------+ Note how it is already filled with the data each CSV file has! The tool supports all kind of files: csv , parquet , json . The application will infer which format to use by looking the file extension. >>> spark.sql(\"SHOW TABLES IN grades\").show() +---------+---------+-----------+ |namespace|tableName|isTemporary| +---------+---------+-----------+ | grades| exams| false| +---------+---------+-----------+ >>> spark.table(\"grades.exams\").show() +---+----------+---------+----------+----+ | id|student_id|course_id| date|note| +---+----------+---------+----------+----+ | 1| 1| 1|2022-05-01| 9| | 2| 2| 1|2022-05-08| 7| | 3| 3| 1|2022-06-17| 4| | 4| 1| 3|2023-05-12| 9| | 5| 2| 3|2023-05-12| 10| | 6| 3| 3|2022-12-07| 7| | 7| 4| 3|2022-12-07| 4| | 8| 5| 3|2022-12-07| 2| | 9| 1| 2|2023-05-01| 5| | 10| 2| 2|2023-05-07| 8| +---+----------+---------+----------+----+ Cleanup Once you finish with your test, you can easily clean the datalake by using the cleanup function and assure that the next test will use a clean environment. >>> builder.cleanup() >>> spark.sql(\"SHOW DATABASES\").show() +---------+ |namespace| +---------+ | default| +---------+","title":"Home"},{"location":"#home","text":"pyspark-data-mocker is a testing tool that facilitates the burden of setting up a desired datalake, so you can test easily the behavior of your data application. It configures also the spark session to optimize it for testing purpose.","title":"Home"},{"location":"#install","text":"pip install pyspark-data-mocker","title":"Install"},{"location":"#basic-usage","text":"pyspark-data-mocker searches the directory you provide in order to seek and load files that can be interpreted as tables, storing them inside the datalake. That datalake will contain certain databases depending on the folders inside the root directory. For example, let's take a look into the basic_datalake $ tree tests/data/basic_datalake -n --charset=ascii # byexample: +rm=~ +skip tests/data/basic_datalake |-- grades | `-- exams.csv `-- school |-- courses.csv `-- students.csv ~ 2 directories, 3 files This file hierarchy will be respected in the further datalake when loaded: each sub-folder will be considered as spark database, and each file will be loaded as table, using the filename to name the table. How can we load them using pyspark-data-mocker ? Really simple! >>> from pyspark_data_mocker import DataLakeBuilder >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/basic_datalake\") # byexample: +timeout=20 +pass And that's it! you will now have in that execution context a datalake with the structure defined in the folder basic_datalake . Let's take a closer look by running some queries. >>> from pyspark.sql import SparkSession >>> spark = SparkSession.builder.getOrCreate() >>> spark.sql(\"SHOW DATABASES\").show() +---------+ |namespace| +---------+ | default| | grades| | school| +---------+ We have the default database (which came for free when instantiating spark), and the two folders inside tests/data/basic_datalake : school and grades . >>> spark.sql(\"SHOW TABLES IN school\").show() +---------+---------+-----------+ |namespace|tableName|isTemporary| +---------+---------+-----------+ | school| courses| false| | school| students| false| +---------+---------+-----------+ >>> spark.sql(\"SELECT * FROM school.courses\").show() +---+------------+ | id| course_name| +---+------------+ | 1|Algorithms 1| | 2|Algorithms 2| | 3| Calculus 1| +---+------------+ >>> spark.table(\"school.students\").show() +---+----------+---------+--------------------+------+----------+ | id|first_name|last_name| email|gender|birth_date| +---+----------+---------+--------------------+------+----------+ | 1| Shirleen| Dunford|sdunford0@amazona...|Female|1978-08-01| | 2| Niko| Puckrin|npuckrin1@shinyst...| Male|2000-11-28| | 3| Sergei| Barukh|sbarukh2@bizjourn...| Male|1992-01-20| | 4| Sal| Maidens|smaidens3@senate.gov| Male|2003-12-14| | 5| Cooper|MacGuffie| cmacguffie4@ibm.com| Male|2000-03-07| +---+----------+---------+--------------------+------+----------+ Note how it is already filled with the data each CSV file has! The tool supports all kind of files: csv , parquet , json . The application will infer which format to use by looking the file extension. >>> spark.sql(\"SHOW TABLES IN grades\").show() +---------+---------+-----------+ |namespace|tableName|isTemporary| +---------+---------+-----------+ | grades| exams| false| +---------+---------+-----------+ >>> spark.table(\"grades.exams\").show() +---+----------+---------+----------+----+ | id|student_id|course_id| date|note| +---+----------+---------+----------+----+ | 1| 1| 1|2022-05-01| 9| | 2| 2| 1|2022-05-08| 7| | 3| 3| 1|2022-06-17| 4| | 4| 1| 3|2023-05-12| 9| | 5| 2| 3|2023-05-12| 10| | 6| 3| 3|2022-12-07| 7| | 7| 4| 3|2022-12-07| 4| | 8| 5| 3|2022-12-07| 2| | 9| 1| 2|2023-05-01| 5| | 10| 2| 2|2023-05-07| 8| +---+----------+---------+----------+----+","title":"Basic usage"},{"location":"#cleanup","text":"Once you finish with your test, you can easily clean the datalake by using the cleanup function and assure that the next test will use a clean environment. >>> builder.cleanup() >>> spark.sql(\"SHOW DATABASES\").show() +---------+ |namespace| +---------+ | default| +---------+","title":"Cleanup"},{"location":"documentation/configuration/","text":"Configuration Default configuration pyspark-data-mocker configures spark in a way that optimize tests executions. >>> from pyspark_data_mocker import DataLakeBuilder >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/basic_datalake\") # byexample: +timeout=20 +pass >>> spark = builder.spark >>> spark_conf = spark.conf >>> spark_conf.get(\"spark.app.name\") 'test' >>> spark_conf.get(\"spark.master\") # 1 thread for the execution 'local[1]' >>> spark_conf.get(\"spark.sql.warehouse.dir\") # Temporal directory to store the data warehouse '/tmp/tmp<...>/spark_warehouse' >>> spark_conf.get(\"spark.sql.shuffle.partitions\") '1' >>> spark_conf.get(\"spark.ui.showConsoleProgress\") 'false' >>> spark_conf.get(\"spark.ui.enabled\") 'false' >>> spark_conf.get(\"spark.ui.dagGraph.retainedRootRDDs\") '1' >>> spark_conf.get(\"spark.ui.retainedJobs\") '1' >>> spark_conf.get(\"spark.ui.retainedStages\") '1' >>> spark_conf.get(\"spark.ui.retainedTasks\") '1' >>> spark_conf.get(\"spark.sql.ui.retainedExecutions\") '1' >>> spark_conf.get(\"spark.worker.ui.retainedExecutors\") '1' >>> spark_conf.get(\"spark.worker.ui.retainedDrivers\") '1' >>> spark_conf.get(\"spark.sql.catalogImplementation\") 'in-memory' To better understand what these configuration means and why it is configured like this, you can take a look on Sergey Ivanychev's excellent research on \"Faster PySpark Unit Test\" Custom configuration Some of these configurations can be overridden by providing a config yaml file. For example lets build a custom configuration. $ echo \" > spark_configuration: > app_name: test_complete > number_of_cores: 4 > enable_hive: True > warehouse_dir: \"/tmp/full_delta_lake\" > delta_configuration: > scala_version: '2.12' > delta_version: '2.0.2' > snapshot_partitions: 2 > log_cache_size: 3 > \" > /tmp/custom_config.yaml To use a custom configuration, you can pass a string or pathlib.Path optional argument to load_from_dir . >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/basic_datalake\", \"/tmp/custom_config.yaml\") # byexample: +timeout=20 <...> >>> spark_conf = SparkSession.builder.getOrCreate().conf >>> spark_conf.get(\"spark.app.name\") 'test_complete' >>> spark_conf.get(\"spark.master\") 'local[4]' >>> spark_conf.get(\"spark.sql.warehouse.dir\") '/tmp/full_delta_lake/spark_warehouse' >>> spark_conf.get(\"spark.jars.packages\") 'io.delta:delta-core_2.12:2.0.2' >>> spark_conf.get(\"spark.sql.extensions\") 'io.delta.sql.DeltaSparkSessionExtension' >>> spark_conf.get(\"spark.databricks.delta.snapshotPartitions\") '2' >>> spark_conf.get(\"spark.sql.catalog.spark_catalog\") 'org.apache.spark.sql.delta.catalog.DeltaCatalog' >>> spark_conf.get(\"spark.sql.catalogImplementation\") 'hive' Note that now the spark session now use 4 CPU cores, the delta framework is enabled, and it uses the hive catalog implementation. Configuration file explanation \u00bfBut, what do those values represent? Let's take a closer look on the levers that we can control in this configuration file App configuration config name type default value description schema SCHEMA_CONFIG DEFAULT_CONFIG Schema configuration disable_spark_configuration BOOL False If set to true, then all spark optimization mentioned before will be disabled. It is responsability of the developer to configure spark as he wish spark_configuration SPARK_CONFIG A reduced a mount of levers to modify the spark configuration recommended for tests. Schema configuration Inside the app configuration, there is a special configuration for the schema. There you can set these options as you please config name type default description infer BOOL false Enable automatic column type infering config_file STRING schema_config.yaml Config file name to read for manual schema definition More about schema inferring can be seen here Spark configuration This is mandatory if you want to let pyspark-data-mocker to handle the spark configuration for you. It tries to abstract the user how the session should be and let him concentrate on define good test. One advance developer may ask by himself: \"why are you obfuscating me how the spark session is configured? I demand to be free to configure Spark as I desire\". To that advance user , you can disable the automatic spark configuration by setting as True the value disable_spark_configuration . That Engineer is responsible to configure spark before using this package, using the jars he wants. Here we recommend to still stick to the recommendations commented here in order to make the tests as fast as possible. Take in mind that the default spark configuration behaves poorly when handling a low amount of data, and if you write a considerable amount of test, the pipeline that run all the unit test may take forever!. Among the things you can modify in the Spark session if you let this package control it are: config name type default value description number_of_cores INTEGER 1 change the amount of CPU cores The spark session will use enable_hive BOOL false Enables the usage of Apache Hive's catalog warehouse_dir STRING tempfile.TemporaryDirectory() If set, it will create a persistent directory where the wharehouse will live. By default pyspark_data_mocker uses a TemporaryDirectory that will exists as long the builder instance exists delta_configuration DELTA_CONFIG None If set, it will enable Delta Lake framework Delta configuration Among the things you can change when enabling Delta capabilities are: config name type description scala_version STRING Version of Scala that the spark session will use. Take into consideration that the scala version MUST be compatible with the Delta-core version used delta_version STRING Version of delta core used. The version used highly depends on the pyspark version snapshot_partitions INTEGER Tells delta how should the partitions be done log_cache_size INTEGER Limits the Delta log cache Important note: If you enable Delta capabilities, check your pyspark version, and configure the right value of scala and delta version. Important note 2: For the delta configuration, take into consideration that ALL VALUES should be explicitly set-up, there is no default value for each one of them.","title":"Configuration"},{"location":"documentation/configuration/#configuration","text":"","title":"Configuration"},{"location":"documentation/configuration/#default-configuration","text":"pyspark-data-mocker configures spark in a way that optimize tests executions. >>> from pyspark_data_mocker import DataLakeBuilder >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/basic_datalake\") # byexample: +timeout=20 +pass >>> spark = builder.spark >>> spark_conf = spark.conf >>> spark_conf.get(\"spark.app.name\") 'test' >>> spark_conf.get(\"spark.master\") # 1 thread for the execution 'local[1]' >>> spark_conf.get(\"spark.sql.warehouse.dir\") # Temporal directory to store the data warehouse '/tmp/tmp<...>/spark_warehouse' >>> spark_conf.get(\"spark.sql.shuffle.partitions\") '1' >>> spark_conf.get(\"spark.ui.showConsoleProgress\") 'false' >>> spark_conf.get(\"spark.ui.enabled\") 'false' >>> spark_conf.get(\"spark.ui.dagGraph.retainedRootRDDs\") '1' >>> spark_conf.get(\"spark.ui.retainedJobs\") '1' >>> spark_conf.get(\"spark.ui.retainedStages\") '1' >>> spark_conf.get(\"spark.ui.retainedTasks\") '1' >>> spark_conf.get(\"spark.sql.ui.retainedExecutions\") '1' >>> spark_conf.get(\"spark.worker.ui.retainedExecutors\") '1' >>> spark_conf.get(\"spark.worker.ui.retainedDrivers\") '1' >>> spark_conf.get(\"spark.sql.catalogImplementation\") 'in-memory' To better understand what these configuration means and why it is configured like this, you can take a look on Sergey Ivanychev's excellent research on \"Faster PySpark Unit Test\"","title":"Default configuration"},{"location":"documentation/configuration/#custom-configuration","text":"Some of these configurations can be overridden by providing a config yaml file. For example lets build a custom configuration. $ echo \" > spark_configuration: > app_name: test_complete > number_of_cores: 4 > enable_hive: True > warehouse_dir: \"/tmp/full_delta_lake\" > delta_configuration: > scala_version: '2.12' > delta_version: '2.0.2' > snapshot_partitions: 2 > log_cache_size: 3 > \" > /tmp/custom_config.yaml To use a custom configuration, you can pass a string or pathlib.Path optional argument to load_from_dir . >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/basic_datalake\", \"/tmp/custom_config.yaml\") # byexample: +timeout=20 <...> >>> spark_conf = SparkSession.builder.getOrCreate().conf >>> spark_conf.get(\"spark.app.name\") 'test_complete' >>> spark_conf.get(\"spark.master\") 'local[4]' >>> spark_conf.get(\"spark.sql.warehouse.dir\") '/tmp/full_delta_lake/spark_warehouse' >>> spark_conf.get(\"spark.jars.packages\") 'io.delta:delta-core_2.12:2.0.2' >>> spark_conf.get(\"spark.sql.extensions\") 'io.delta.sql.DeltaSparkSessionExtension' >>> spark_conf.get(\"spark.databricks.delta.snapshotPartitions\") '2' >>> spark_conf.get(\"spark.sql.catalog.spark_catalog\") 'org.apache.spark.sql.delta.catalog.DeltaCatalog' >>> spark_conf.get(\"spark.sql.catalogImplementation\") 'hive' Note that now the spark session now use 4 CPU cores, the delta framework is enabled, and it uses the hive catalog implementation.","title":"Custom configuration"},{"location":"documentation/configuration/#configuration-file-explanation","text":"\u00bfBut, what do those values represent? Let's take a closer look on the levers that we can control in this configuration file","title":"Configuration file explanation"},{"location":"documentation/configuration/#app-configuration","text":"config name type default value description schema SCHEMA_CONFIG DEFAULT_CONFIG Schema configuration disable_spark_configuration BOOL False If set to true, then all spark optimization mentioned before will be disabled. It is responsability of the developer to configure spark as he wish spark_configuration SPARK_CONFIG A reduced a mount of levers to modify the spark configuration recommended for tests.","title":"App configuration"},{"location":"documentation/configuration/#schema-configuration","text":"Inside the app configuration, there is a special configuration for the schema. There you can set these options as you please config name type default description infer BOOL false Enable automatic column type infering config_file STRING schema_config.yaml Config file name to read for manual schema definition More about schema inferring can be seen here","title":"Schema configuration"},{"location":"documentation/configuration/#spark-configuration","text":"This is mandatory if you want to let pyspark-data-mocker to handle the spark configuration for you. It tries to abstract the user how the session should be and let him concentrate on define good test. One advance developer may ask by himself: \"why are you obfuscating me how the spark session is configured? I demand to be free to configure Spark as I desire\". To that advance user , you can disable the automatic spark configuration by setting as True the value disable_spark_configuration . That Engineer is responsible to configure spark before using this package, using the jars he wants. Here we recommend to still stick to the recommendations commented here in order to make the tests as fast as possible. Take in mind that the default spark configuration behaves poorly when handling a low amount of data, and if you write a considerable amount of test, the pipeline that run all the unit test may take forever!. Among the things you can modify in the Spark session if you let this package control it are: config name type default value description number_of_cores INTEGER 1 change the amount of CPU cores The spark session will use enable_hive BOOL false Enables the usage of Apache Hive's catalog warehouse_dir STRING tempfile.TemporaryDirectory() If set, it will create a persistent directory where the wharehouse will live. By default pyspark_data_mocker uses a TemporaryDirectory that will exists as long the builder instance exists delta_configuration DELTA_CONFIG None If set, it will enable Delta Lake framework","title":"Spark configuration"},{"location":"documentation/configuration/#delta-configuration","text":"Among the things you can change when enabling Delta capabilities are: config name type description scala_version STRING Version of Scala that the spark session will use. Take into consideration that the scala version MUST be compatible with the Delta-core version used delta_version STRING Version of delta core used. The version used highly depends on the pyspark version snapshot_partitions INTEGER Tells delta how should the partitions be done log_cache_size INTEGER Limits the Delta log cache Important note: If you enable Delta capabilities, check your pyspark version, and configure the right value of scala and delta version. Important note 2: For the delta configuration, take into consideration that ALL VALUES should be explicitly set-up, there is no default value for each one of them.","title":"Delta configuration"},{"location":"documentation/schema_infering/","text":"Schema inferring pyspark-data-mocker lets you define the schema of the table as you please. You can enable automatic schema inferring by setting up the schema.infer option in the configuration file , or you can manually specify the schema of each column you want using another yaml file. By default, pyspark-data-mocker will consider that all columns are string columns Automatic inferring This is the simplest configuration. Let's see the example we saw before in the welcome page with automatic infer schema enabled $ cat ./tests/data/config/infer_schema.yaml spark_configuration: app_name: \"test\" number_of_cores: 1 schema: infer: True You only need to set the boolean schema.infer to True and that is it! once you load the builder, the columns will vary depending on their values >>> from pyspark_data_mocker import DataLakeBuilder >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/basic_datalake\", \"./tests/data/config/infer_schema.yaml\") # byexample: +timeout=20 +pass >>> spark = builder.spark >>> spark.sql(\"DESCRIBE TABLE school.students\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| int| |first_name| string| | last_name| string| | email| string| | gender| string| |birth_date| string| +----------+---------+ >>> spark.sql(\"DESCRIBE TABLE school.courses\").select(\"col_name\", \"data_type\").show() +-----------+---------+ | col_name|data_type| +-----------+---------+ | id| int| |course_name| string| +-----------+---------+ >>> spark.sql(\"DESCRIBE TABLE grades.exams\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| int| |student_id| int| | course_id| int| | date| string| | note| int| +----------+---------+ >>> builder.cleanup() Schema configuration file This yaml file needs to be located in the folder you will place the datalake definition (the root path you will pass to the DatalakeBuilder class). By default, the config file that will be used is called schema_config.yaml , but it can be overriden in the application configuration file . $ cat ./pyspark_data_mocker/config/schema.py <...>schema.Optional(\"config_file\", default=\"schema_config.yaml\")<...> That yaml needs to be a file where each key represents the table name (considering the database), and as value, a dictionary with the columns as keys, and a Spark's DDL type of the column as value. Example Let's consider this datalake definition. $ tree tests/data/datalake_with_config_schema -n --charset=ascii # byexample: +rm=~ +skip tests/data/datalake_with_config_schema |-- grades | `-- exams.csv |-- schema_config.yaml `-- school |-- courses.csv `-- students.csv ~ 2 directories, 4 files Notice how in this example, unlike the one seen previously in the Home section contains a file schema_config.yaml . The content of this file will define the types of each column of the tables. $ cat tests/data/datalake_with_config_schema/schema_config.yaml school.courses: id: int course_name: string school.students: id: int first_name: string last_name: string email: string gender: string birth_date: date Take a moment to digest the schema of the file. How each key of the yaml dictionary is the full name of the table that will be created, and as value contains another dictionary with the columns of the table, and the type of that column. Let's build up the datalake >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/datalake_with_config_schema\") # byexample: +timeout=20 +pass >>> spark = builder.spark >>> spark.sql(\"SHOW TABLES IN school\").show() +---------+---------+-----------+ |namespace|tableName|isTemporary| +---------+---------+-----------+ | school| courses| false| | school| students| false| +---------+---------+-----------+ >>> spark.sql(\"SHOW TABLES IN grades\").show() +---------+---------+-----------+ |namespace|tableName|isTemporary| +---------+---------+-----------+ | grades| exams| false| +---------+---------+-----------+ Now the tables are loaded, we can take a look at the schema of each table. >>> spark.sql(\"DESCRIBE TABLE school.students\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| int| |first_name| string| | last_name| string| | email| string| | gender| string| |birth_date| date| +----------+---------+ >>> spark.sql(\"DESCRIBE TABLE school.courses\").select(\"col_name\", \"data_type\").show() +-----------+---------+ | col_name|data_type| +-----------+---------+ | id| int| |course_name| string| +-----------+---------+ >>> spark.sql(\"DESCRIBE TABLE grades.exams\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| string| |student_id| string| | course_id| string| | date| string| | note| string| +----------+---------+ >>> builder.cleanup() Now the column types changed! we have the birth_date that is date type and the ids as int . Notice also that the table grades.exams (which we didn't define any custom schema) has for each column the default value string (because it's the fallback type as we saw before). Combining both schema inferring configurations We can combine this file with the automatic infer option to only configure manually the schemas that we need. >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/datalake_with_config_schema\", \"./tests/data/config/infer_schema.yaml\") # byexample: +timeout=20 +pass >>> spark = builder.spark >>> spark.sql(\"DESCRIBE TABLE school.students\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| int| |first_name| string| | last_name| string| | email| string| | gender| string| |birth_date| date| +----------+---------+ >>> spark.sql(\"DESCRIBE TABLE school.courses\").select(\"col_name\", \"data_type\").show() +-----------+---------+ | col_name|data_type| +-----------+---------+ | id| int| |course_name| string| +-----------+---------+ >>> spark.sql(\"DESCRIBE TABLE grades.exams\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| int| |student_id| int| | course_id| int| | date| string| | note| int| +----------+---------+ >>> builder.cleanup() Now the grades.exams table schema also changed! but take into consideration that the automatic schema inferring of spark it's not magic . Note that the date column of grades.exams was not inferred to a date column type. Sometimes it is needed to use the manual schema definition to have the value we need. NOTE : This behavior is fixed starting from pyspark 3.3. From that version and beyond, it infers date columns, but spark considers all date-kind values as datetime Column types You can define the type of column of each type that Spark supports ! you don't have any restriction whatsoever (kind of, but more of that later). Example Consider these files and schema definitions $ tree tests/data/datalake_different_files_and_schemas -n --charset=ascii # byexample: +rm=~ +skip tests/data/datalake_different_files_and_schemas |-- schema_config.yaml `-- school |-- courses.json `-- students.csv ~ 1 directory, 3 files $ cat tests/data/datalake_different_files_and_schemas/school/courses.json {\"id\": 1, \"course_name\": \"Algorithms 1\", \"flags\": {\"acitve\": true}, \"correlative_courses\": []} {\"id\": 2, \"course_name\": \"Algorithms 2\", \"flags\": {\"acitve\": true}, \"correlative_courses\": [1]} $ cat tests/data/datalake_different_files_and_schemas/schema_config.yaml school.courses: id: int course_name: string flags: map<string, boolean> correlative_courses: array<int> school.students: id: long name: string birth_date: date pyspark-data-mocker does not need that all files are in the same file format, it will infer each file depending on the file extension (the limitation is that the file format is a valid spark source). Let's see the schemas and data in each table. >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/datalake_different_files_and_schemas\") # byexample: +timeout=20 +pass >>> spark = builder.spark >>> spark.sql(\"DESCRIBE TABLE school.students\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| bigint| | name| string| |birth_date| date| +----------+---------+ >>> spark.table(\"school.students\").show() +---+----------------+----------+ | id| name|birth_date| +---+----------------+----------+ | 1|Shirleen Dunford|1978-08-01| | 2| Niko Puckrin|2000-11-28| | 3| Sergei Barukh|1992-01-20| | 4| Sal Maidens|2003-12-14| | 5|Cooper MacGuffie|2000-03-07| +---+----------------+----------+ >>> spark.sql(\"DESCRIBE TABLE school.courses\").select(\"col_name\", \"data_type\").show() +-------------------+-------------------+ | col_name| data_type| +-------------------+-------------------+ | id| int| | course_name| string| | flags|map<string,boolean>| |correlative_courses| array<int>| +-------------------+-------------------+ >>> spark.table(\"school.courses\").show() +---+------------+----------------+-------------------+ | id| course_name| flags|correlative_courses| +---+------------+----------------+-------------------+ | 1|Algorithms 1|{acitve -> true}| []| | 2|Algorithms 2|{acitve -> true}| [1]| +---+------------+----------------+-------------------+ >>> builder.cleanup() Limitations The limitation you have is that you cannot define certain column types in some file formats. An example of that is to define a map column type in a csv file. $ cat tests/data/column_type_not_supported/schema_config.yaml foo.bar: id: int invalid_col: map<string, string> $ tree tests/data/column_type_not_supported -n --charset=ascii # byexample: +rm=~ +skip tests/data/column_type_not_supported |-- foo | `-- bar.csv `-- schema_config.yaml ~ 1 directory, 2 files If we want to set up this datalake, it will fail with this exception message. >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/column_type_not_supported\") # byexample: +timeout=20 <...>AnalysisException: CSV data source does not support map<string,string> data type.","title":"Schema infering"},{"location":"documentation/schema_infering/#schema-inferring","text":"pyspark-data-mocker lets you define the schema of the table as you please. You can enable automatic schema inferring by setting up the schema.infer option in the configuration file , or you can manually specify the schema of each column you want using another yaml file. By default, pyspark-data-mocker will consider that all columns are string columns","title":"Schema inferring"},{"location":"documentation/schema_infering/#automatic-inferring","text":"This is the simplest configuration. Let's see the example we saw before in the welcome page with automatic infer schema enabled $ cat ./tests/data/config/infer_schema.yaml spark_configuration: app_name: \"test\" number_of_cores: 1 schema: infer: True You only need to set the boolean schema.infer to True and that is it! once you load the builder, the columns will vary depending on their values >>> from pyspark_data_mocker import DataLakeBuilder >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/basic_datalake\", \"./tests/data/config/infer_schema.yaml\") # byexample: +timeout=20 +pass >>> spark = builder.spark >>> spark.sql(\"DESCRIBE TABLE school.students\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| int| |first_name| string| | last_name| string| | email| string| | gender| string| |birth_date| string| +----------+---------+ >>> spark.sql(\"DESCRIBE TABLE school.courses\").select(\"col_name\", \"data_type\").show() +-----------+---------+ | col_name|data_type| +-----------+---------+ | id| int| |course_name| string| +-----------+---------+ >>> spark.sql(\"DESCRIBE TABLE grades.exams\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| int| |student_id| int| | course_id| int| | date| string| | note| int| +----------+---------+ >>> builder.cleanup()","title":"Automatic inferring"},{"location":"documentation/schema_infering/#schema-configuration-file","text":"This yaml file needs to be located in the folder you will place the datalake definition (the root path you will pass to the DatalakeBuilder class). By default, the config file that will be used is called schema_config.yaml , but it can be overriden in the application configuration file . $ cat ./pyspark_data_mocker/config/schema.py <...>schema.Optional(\"config_file\", default=\"schema_config.yaml\")<...> That yaml needs to be a file where each key represents the table name (considering the database), and as value, a dictionary with the columns as keys, and a Spark's DDL type of the column as value.","title":"Schema configuration file"},{"location":"documentation/schema_infering/#example","text":"Let's consider this datalake definition. $ tree tests/data/datalake_with_config_schema -n --charset=ascii # byexample: +rm=~ +skip tests/data/datalake_with_config_schema |-- grades | `-- exams.csv |-- schema_config.yaml `-- school |-- courses.csv `-- students.csv ~ 2 directories, 4 files Notice how in this example, unlike the one seen previously in the Home section contains a file schema_config.yaml . The content of this file will define the types of each column of the tables. $ cat tests/data/datalake_with_config_schema/schema_config.yaml school.courses: id: int course_name: string school.students: id: int first_name: string last_name: string email: string gender: string birth_date: date Take a moment to digest the schema of the file. How each key of the yaml dictionary is the full name of the table that will be created, and as value contains another dictionary with the columns of the table, and the type of that column. Let's build up the datalake >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/datalake_with_config_schema\") # byexample: +timeout=20 +pass >>> spark = builder.spark >>> spark.sql(\"SHOW TABLES IN school\").show() +---------+---------+-----------+ |namespace|tableName|isTemporary| +---------+---------+-----------+ | school| courses| false| | school| students| false| +---------+---------+-----------+ >>> spark.sql(\"SHOW TABLES IN grades\").show() +---------+---------+-----------+ |namespace|tableName|isTemporary| +---------+---------+-----------+ | grades| exams| false| +---------+---------+-----------+ Now the tables are loaded, we can take a look at the schema of each table. >>> spark.sql(\"DESCRIBE TABLE school.students\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| int| |first_name| string| | last_name| string| | email| string| | gender| string| |birth_date| date| +----------+---------+ >>> spark.sql(\"DESCRIBE TABLE school.courses\").select(\"col_name\", \"data_type\").show() +-----------+---------+ | col_name|data_type| +-----------+---------+ | id| int| |course_name| string| +-----------+---------+ >>> spark.sql(\"DESCRIBE TABLE grades.exams\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| string| |student_id| string| | course_id| string| | date| string| | note| string| +----------+---------+ >>> builder.cleanup() Now the column types changed! we have the birth_date that is date type and the ids as int . Notice also that the table grades.exams (which we didn't define any custom schema) has for each column the default value string (because it's the fallback type as we saw before).","title":"Example"},{"location":"documentation/schema_infering/#combining-both-schema-inferring-configurations","text":"We can combine this file with the automatic infer option to only configure manually the schemas that we need. >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/datalake_with_config_schema\", \"./tests/data/config/infer_schema.yaml\") # byexample: +timeout=20 +pass >>> spark = builder.spark >>> spark.sql(\"DESCRIBE TABLE school.students\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| int| |first_name| string| | last_name| string| | email| string| | gender| string| |birth_date| date| +----------+---------+ >>> spark.sql(\"DESCRIBE TABLE school.courses\").select(\"col_name\", \"data_type\").show() +-----------+---------+ | col_name|data_type| +-----------+---------+ | id| int| |course_name| string| +-----------+---------+ >>> spark.sql(\"DESCRIBE TABLE grades.exams\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| int| |student_id| int| | course_id| int| | date| string| | note| int| +----------+---------+ >>> builder.cleanup() Now the grades.exams table schema also changed! but take into consideration that the automatic schema inferring of spark it's not magic . Note that the date column of grades.exams was not inferred to a date column type. Sometimes it is needed to use the manual schema definition to have the value we need. NOTE : This behavior is fixed starting from pyspark 3.3. From that version and beyond, it infers date columns, but spark considers all date-kind values as datetime","title":"Combining both schema inferring configurations"},{"location":"documentation/schema_infering/#column-types","text":"You can define the type of column of each type that Spark supports ! you don't have any restriction whatsoever (kind of, but more of that later).","title":"Column types"},{"location":"documentation/schema_infering/#example_1","text":"Consider these files and schema definitions $ tree tests/data/datalake_different_files_and_schemas -n --charset=ascii # byexample: +rm=~ +skip tests/data/datalake_different_files_and_schemas |-- schema_config.yaml `-- school |-- courses.json `-- students.csv ~ 1 directory, 3 files $ cat tests/data/datalake_different_files_and_schemas/school/courses.json {\"id\": 1, \"course_name\": \"Algorithms 1\", \"flags\": {\"acitve\": true}, \"correlative_courses\": []} {\"id\": 2, \"course_name\": \"Algorithms 2\", \"flags\": {\"acitve\": true}, \"correlative_courses\": [1]} $ cat tests/data/datalake_different_files_and_schemas/schema_config.yaml school.courses: id: int course_name: string flags: map<string, boolean> correlative_courses: array<int> school.students: id: long name: string birth_date: date pyspark-data-mocker does not need that all files are in the same file format, it will infer each file depending on the file extension (the limitation is that the file format is a valid spark source). Let's see the schemas and data in each table. >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/datalake_different_files_and_schemas\") # byexample: +timeout=20 +pass >>> spark = builder.spark >>> spark.sql(\"DESCRIBE TABLE school.students\").select(\"col_name\", \"data_type\").show() +----------+---------+ | col_name|data_type| +----------+---------+ | id| bigint| | name| string| |birth_date| date| +----------+---------+ >>> spark.table(\"school.students\").show() +---+----------------+----------+ | id| name|birth_date| +---+----------------+----------+ | 1|Shirleen Dunford|1978-08-01| | 2| Niko Puckrin|2000-11-28| | 3| Sergei Barukh|1992-01-20| | 4| Sal Maidens|2003-12-14| | 5|Cooper MacGuffie|2000-03-07| +---+----------------+----------+ >>> spark.sql(\"DESCRIBE TABLE school.courses\").select(\"col_name\", \"data_type\").show() +-------------------+-------------------+ | col_name| data_type| +-------------------+-------------------+ | id| int| | course_name| string| | flags|map<string,boolean>| |correlative_courses| array<int>| +-------------------+-------------------+ >>> spark.table(\"school.courses\").show() +---+------------+----------------+-------------------+ | id| course_name| flags|correlative_courses| +---+------------+----------------+-------------------+ | 1|Algorithms 1|{acitve -> true}| []| | 2|Algorithms 2|{acitve -> true}| [1]| +---+------------+----------------+-------------------+ >>> builder.cleanup()","title":"Example"},{"location":"documentation/schema_infering/#limitations","text":"The limitation you have is that you cannot define certain column types in some file formats. An example of that is to define a map column type in a csv file. $ cat tests/data/column_type_not_supported/schema_config.yaml foo.bar: id: int invalid_col: map<string, string> $ tree tests/data/column_type_not_supported -n --charset=ascii # byexample: +rm=~ +skip tests/data/column_type_not_supported |-- foo | `-- bar.csv `-- schema_config.yaml ~ 1 directory, 2 files If we want to set up this datalake, it will fail with this exception message. >>> builder = DataLakeBuilder.load_from_dir(\"./tests/data/column_type_not_supported\") # byexample: +timeout=20 <...>AnalysisException: CSV data source does not support map<string,string> data type.","title":"Limitations"}]}